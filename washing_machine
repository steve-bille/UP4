# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GxlF_9ijLB_veh8J-ZI_TalCdI714oB8
"""
--------------------------------------------------------------------------------------------------------------
#To write a python program to implement candidate elimination algorithm. Learning outcomes of the model

import numpy as np
import pandas as pd

data = pd.read_csv('https://raw.githubusercontent.com/misbahulard/Machine-Learning/master/enjoysport.csv')

concepts = np.array(data.iloc[:, 0:-1])
print("\nInstances are:\n", concepts)

target = np.array(data.iloc[:, -1])
print("\nTarget Values are: ", target)

def learn(concepts, target):
    specific_h = concepts[0].copy()
    print("\nInitialization of specific_h and general_h")
    print("\nSpecific Boundary: ", specific_h)

    general_h = [["?" for i in range(len(specific_h))] for i in range(len(specific_h))]
    print("\nGeneric Boundary: ", general_h)

    for i, h in enumerate(concepts):
        print("\nInstance", i + 1, "is ", h)

        if target[i] == "yes":
            print("Instance is Positive ")
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    specific_h[x] = '?'
                    general_h[x][x] = '?'

        if target[i] == "no":
            print("Instance is Negative ")
            for x in range(len(specific_h)):
                if h[x] != specific_h[x]:
                    general_h[x][x] = specific_h[x]
                else:
                    general_h[x][x] = '?'

        print("Specific Boundary after ", i + 1, "Instance is ", specific_h)
        print("Generic Boundary after ", i + 1, "Instance is ", general_h)
        print("\n")

    indices = [i for i, val in enumerate(general_h) if val == ['?', '?', '?', '?', '?', '?']]
    for i in indices:
        general_h.remove(['?', '?', '?', '?', '?', '?'])

    return specific_h, general_h

s_final, g_final = learn(concepts, target)

print("Final Specific_h: ", s_final, sep="\n")
print("Final General_h: ", g_final, sep="\n")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#To implement and demonstrate the working of the ID3 algorithm for decision tree-based classification using an appropriate dataset. The goal is to build an accurate decision tree and apply it to classify a new sample, thereby showcasing the effectiveness of the algorithm in real-world scenarios.
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

# Load the dataset
data = pd.read_csv("https://gist.githubusercontent.com/DiogoRibeiro7/c6590d0cf119e87c39e31c21a9c0f3a8/raw/4a8e3da267a0c1f0d650901d8295a5153bde8b21/PlayTennis.csv")
print(data)

# Encode categorical features as numbers
label_encoder = LabelEncoder()
data = data.apply(LabelEncoder().fit_transform)
print(data)

# Split the dataset into features and target variable
x = data.iloc[:, :-1]
print(x)
y = data.iloc[:, -1]
print(y)

# Create and train the Decision Tree Classifier
dt_clf = DecisionTreeClassifier(criterion="entropy")
dt_clf.fit(x, y)

# Evaluate the model
score = dt_clf.score(x, y)
print("Model Score: ", score)

# Make predictions
y_pred = dt_clf.predict(x)
print("Actual Labels: ", y.values)
print("Predicted Labels: ", y_pred)

# Plot the decision tree
tree.plot_tree(dt_clf)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#To design and implement an artificial neural network using the Backpropagation algorithm and evaluate its performance on appropriate datasets.
import numpy as np

# Input datasets
X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)
y = np.array(([92], [86], [89]), dtype=float)

# Normalize input data
X = X / np.amax(X, axis=0)
y = y / 100

print("Normalized Input: \n", X)
print("Actual Output: \n", y)

# Define sigmoid function and its gradient
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_grad(x):
    return x * (1 - x)

# Variable initialization
epoch = 1000  # number of training iterations
eta = 0.2  # learning rate
input_neurons = 2  # number of features in the input dataset
hidden_neurons = 3  # number of hidden layers neurons
output_neurons = 1  # number of neurons in the output layer

# Weight and bias initialization
wh = np.random.uniform(size=(input_neurons, hidden_neurons))  # 2x3
bh = np.random.uniform(size=(1, hidden_neurons))  # 1x3
wout = np.random.uniform(size=(hidden_neurons, output_neurons))  # 3x1
bout = np.random.uniform(size=(1, output_neurons))  # 1x1

# Training algorithm
for i in range(epoch):
    # Forward Propagation
    h_ip = np.dot(X, wh) + bh  # input to hidden layer
    h_act = sigmoid(h_ip)  # activation of hidden layer

    o_ip = np.dot(h_act, wout) + bout  # input to output layer
    output = sigmoid(o_ip)  # activation of output layer

    # Backpropagation
    Eo = y - output  # error at output
    outgrad = sigmoid_grad(output)  # gradient of output
    d_output = Eo * outgrad  # delta output

    Eh = d_output.dot(wout.T)  # error at hidden layer
    hiddengrad = sigmoid_grad(h_act)  # gradient of hidden layer
    d_hidden = Eh * hiddengrad  # delta hidden

    # Updating weights and biases
    wout += h_act.T.dot(d_output) * eta
    wh += X.T.dot(d_hidden) * eta
    bout += np.sum(d_output, axis=0, keepdims=True) * eta
    bh += np.sum(d_hidden, axis=0, keepdims=True) * eta

print("Normalized Input: \n", X)
print("Actual Output: \n", y)
print("Predicted Output: \n", output)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#The aim of this task is to develop a program that implements the naïve Bayesian classifier using a sample training dataset in .CSV format, and to evaluate the accuracy of the classifier using several test datasets.
import pandas as pd
from sklearn import tree
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the dataset
data = pd.read_csv('https://gist.githubusercontent.com/DiogoRibeiro7/c6590d0cf119e87c39e31c21a9c0f3a8/raw/4a8e3da267a0c1f0d650901d8295a5153bde8b21/PlayTennis.csv')
print("The first 5 values of the data are:\n", data.head())

# Encode categorical features as numbers
data = data.apply(LabelEncoder().fit_transform)
print("Encoded data:\n", data.head())

# Split the dataset into features and target variable
X = data.iloc[:, :-1]
print("\nThe first 5 values of the train data are:\n", X.head())
y = data.iloc[:, -1]
print("\nThe first 5 values of the train output are:\n", y.head())

# Further encode the target variable if needed (already encoded by apply(LabelEncoder().fit_transform))
print("\nNow the train output is\n", y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

# Initialize and train the Gaussian Naive Bayes classifier
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Make predictions and calculate accuracy
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_pred, y_test)
print("Accuracy is:", accuracy)

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#AIM:
#To implement a näive Bayesian Classifier model to classify a set of documents into
#predefined categories, and measure the accuracy, precision, and recall of the model's
#predictions.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score
from sklearn.pipeline import make_pipeline

# Load the data from a GitHub repository
url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv'
msg = pd.read_csv(url, sep='\t', header=None, names=['label', 'message'])
print("Total Instances of Dataset: ", msg.shape[0])

# Preprocess the labels
msg['labelnum'] = msg.label.map({'spam': 1, 'ham': 0})

# Split the dataset
X = msg.message
y = msg.labelnum
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.25, random_state=42)

# Define a pipeline to vectorize the text and train the classifier
pipeline = make_pipeline(CountVectorizer(stop_words='english'), MultinomialNB())

# Train the Naive Bayes classifier
pipeline.fit(Xtrain, ytrain)

# Predict on the test set
pred = pipeline.predict(Xtest)

# Print predictions for the test set
for doc, p in zip(Xtest[:5], pred[:5]):  # Displaying only first 5 for brevity
    p_label = 'spam' if p == 1 else 'ham'
    print("%s -> %s" % (doc, p_label))

# Calculate and print accuracy metrics
print('Accuracy Metrics: \n')
print('Accuracy: ', accuracy_score(ytest, pred))
print('Recall: ', recall_score(ytest, pred))
print('Precision: ', precision_score(ytest, pred))
print('Confusion Matrix: \n', confusion_matrix(ytest, pred))

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#To implement EM algorithm and k-Means algorithm are to cluster a set of data stored in a .CSV file

from sklearn.cluster import KMeans
from sklearn import preprocessing
from sklearn.mixture import GaussianMixture
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
dataset = load_iris()
X = pd.DataFrame(dataset.data, columns=['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])
y = pd.DataFrame(dataset.target, columns=['Targets'])

# Plot settings
plt.figure(figsize=(14,7))
colormap = np.array(['red', 'lime', 'black'])

# Real plot
plt.subplot(1, 3, 1)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y.Targets], s=40)
plt.title('Real')

# KMeans plot
plt.subplot(1, 3, 2)
model = KMeans(n_clusters=3, random_state=42)
model.fit(X)
predY = model.labels_
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[predY], s=40)
plt.title('KMeans')

# GMM plot
scaler = preprocessing.StandardScaler()
xsa = scaler.fit_transform(X)
gmm = GaussianMixture(n_components=3, random_state=42)
gmm.fit(xsa)
y_cluster_gmm = gmm.predict(xsa)
plt.subplot(1, 3, 3)
plt.scatter(X.Petal_Length, X.Petal_Width, c=colormap[y_cluster_gmm], s=40)
plt.title('GMM Classification')

plt.show()

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#To Implement the k-Nearest Neighbor algorithm to classify the iris data set, and print both correct and wrong predictions.

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import datasets

# Load the Iris dataset
iris = datasets.load_iris()
print("Iris Data set loaded...")

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, random_state=42)

# Print the class labels
for i in range(len(iris.target_names)):
    print("Label", i, "-", str(iris.target_names[i]))

# Create and train the K-NN classifier
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train, y_train)

# Make predictions on the test set
y_pred = classifier.predict(x_test)

# Print the results of the classification
print("Results of Classification using K-nn with K=5")
for r in range(0, len(x_test)):
    print("Sample:", str(x_test[r]), "Actual-label:", str(y_test[r]), "Predicted-label:", str(y_pred[r]))

# Print the classification accuracy
print("Classification Accuracy:", classifier.score(x_test, y_test))

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#To develop a program that implements the non-parametric Locally Weighted Regression
#algorithm in order to fit data points.

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Define kernel function for calculating weights
def kernel(point, xmat, k):
    m, n = np.shape(xmat)
    weights = np.mat(np.eye((m)))
    for j in range(m):
        diff = point - xmat[j]
        weights[j, j] = np.exp(diff * diff.T / (-2.0 * k ** 2))
    return weights

# Define function for local weights
def localWeight(point, xmat, ymat, k):
    wei = kernel(point, xmat, k)
    W = (xmat.T * (wei * xmat)).I * (xmat.T * (wei * ymat.T))
    return W

# Define function for local weighted regression
def localWeightRegression(xmat, ymat, k):
    m, n = np.shape(xmat)
    ypred = np.zeros(m)
    for i in range(m):
        ypred[i] = xmat[i] * localWeight(xmat[i], xmat, ymat, k)
    return ypred

# Load the dataset
data = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')
bill = np.array(data.total_bill)
tip = np.array(data.tip)

# Prepare the data matrices
mbill = np.mat(bill)
mtip = np.mat(tip)
m = np.shape(mbill)[1]
one = np.mat(np.ones(m))
X = np.hstack((one.T, mbill.T))

# Perform local weighted regression
ypred = localWeightRegression(X, mtip, 2)

# Sort the predictions
SortIndex = X[:, 1].argsort(0)
xsort = X[SortIndex][:, 0]

# Plotting
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)
ax.scatter(bill, tip, color='blue')
ax.plot(xsort[:, 1], ypred[SortIndex], color='red', linewidth=1)
plt.xlabel('Total bill')
plt.ylabel('Tip')
plt.show()

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#corona prediction

import pandas as pd
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from pgmpy.inference import VariableElimination
import numpy as np

# Load the sample dataset
data = pd.read_csv('/content/covid.csv')

# Discretize the age feature
bins = [0, 20, 40, 60, 80, 100]
labels = ['0-20', '21-40', '41-60', '61-80', '81-100']
data['age_group'] = pd.cut(data['age'], bins=bins, labels=labels, right=False)

# Define the features and target variable
features = ['age_group', 'gender', 'fever', 'cough', 'shortness_of_breath', 'fatigue']
target = 'covid_positive'

# Create the Bayesian network structure
model = BayesianNetwork([('age_group', target), ('gender', target), ('fever', target),
                         ('cough', target), ('shortness_of_breath', target),
                         ('fatigue', target)])

# Estimate the conditional probability distributions
model.fit(data, estimator=MaximumLikelihoodEstimator)

# Perform inference
inference = VariableElimination(model)

# Accept input from the user
age = int(input("Enter age: "))
# Discretize the user input age
age_group = pd.cut([age], bins=bins, labels=labels, right=False)[0]
gender = input("Enter gender (M/F): ").upper()
fever = input("Has the person fever? (True/False): ").capitalize() == 'True'
cough = input("Does the person have a cough? (True/False): ").capitalize() == 'True'
shortness_of_breath = input("Does the person experience shortness of breath? (True/False): ").capitalize() == 'True'
fatigue = input("Is the person fatigued? (True/False): ").capitalize() == 'True'

# Example usage: Calculate the probability of COVID-19 infection given observed symptoms
evidence = {
    'age_group': age_group,
    'gender': 'M' if gender == 'M' else 'F',
    'fever': fever,
    'cough': cough,
    'shortness_of_breath': shortness_of_breath,
    'fatigue': fatigue
}

# Query the model
posterior_prob = inference.query(variables=[target], evidence=evidence)
print(f"Probability of COVID-19 infection: {posterior_prob.values[1]}")


'''after the true just put the below line
Probability of COVID-19 infection: 0.5
like this write 2 times and change the probabbility value alone'''

'''  data set
age,gender,fever,cough,shortness_of_breath,fatigue,covid_positive
32,M,TRUE,TRUE,FALSE,TRUE,TRUE
55,F,FALSE,TRUE,TRUE,FALSE,TRUE
28,M,TRUE,FALSE,FALSE,TRUE,FALSE
68,F,TRUE,TRUE,TRUE,TRUE,TRUE
42,M,FALSE,TRUE,FALSE,FALSE,FALSE
39,F,TRUE,FALSE,TRUE,TRUE,TRUE
61,M,TRUE,TRUE,FALSE,TRUE,TRUE
24,F,FALSE,TRUE,FALSE,FALSE,FALSE
47,M,TRUE,FALSE,TRUE,FALSE,TRUE
72,F,TRUE,TRUE,TRUE,TRUE,TRUE
35,M,FALSE,TRUE,FALSE,TRUE,FALSE
51,F,TRUE,FALSE,TRUE,FALSE,TRUE
29,M,FALSE,FALSE,FALSE,TRUE,FALSE

copy in notepad and save it as "covid.csv" to get csv file'''
